{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn pandas numpy nltk imblearn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB  # Changed from ComplementNB to MultinomialNB\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, auc, log_loss)\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHDbaLQlD9Oz",
        "outputId": "6bf054bd-2f5f-4e2b-9a74-427ded3d020d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.11/dist-packages (0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMMpnWRQC8ky",
        "outputId": "d686e2b0-5891-4ba6-ebc0-50fef5d9fdb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment 1/25...\n",
            "Running experiment 2/25...\n",
            "Running experiment 3/25...\n",
            "Running experiment 4/25...\n",
            "Running experiment 5/25...\n",
            "Running experiment 6/25...\n",
            "Running experiment 7/25...\n",
            "Running experiment 8/25...\n",
            "Running experiment 9/25...\n",
            "Running experiment 10/25...\n",
            "Running experiment 11/25...\n",
            "Running experiment 12/25...\n",
            "Running experiment 13/25...\n",
            "Running experiment 14/25...\n",
            "Running experiment 15/25...\n",
            "Running experiment 16/25...\n",
            "Running experiment 17/25...\n",
            "Running experiment 18/25...\n",
            "Running experiment 19/25...\n",
            "Running experiment 20/25...\n",
            "Running experiment 21/25...\n",
            "Running experiment 22/25...\n",
            "Running experiment 23/25...\n",
            "Running experiment 24/25...\n",
            "Running experiment 25/25...\n",
            "=== Multinomial Naive Bayes + CountVectorizer Results ===\n",
            "Average Accuracy:      0.8482\n",
            "Average Precision:     0.6634\n",
            "Average Recall:        0.6831\n",
            "Average F1 score:      0.6635\n",
            "Average AUC:           0.7216\n",
            "Results saved to: /content/pytorch_OptimizedNB_Count.csv\n"
          ]
        }
      ],
      "source": [
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "stop_words = set(stopwords.words('english') + ['...', 'bug', 'issue', 'error'])\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word.lower() not in stop_words])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "pd_all = pd.read_csv('/content/pytorch.csv')\n",
        "pd_all = pd_all.sample(frac=1, random_state=999)  # Shuffle\n",
        "pd_all['Title+Body'] = pd_all.apply(\n",
        "    lambda row: row['Title'] + '. ' + row['Body'] if pd.notna(row['Body']) else row['Title'], axis=1\n",
        ")\n",
        "data = pd_all.rename(columns={\"Unnamed: 0\": \"id\", \"class\": \"sentiment\", \"Title+Body\": \"text\"})\n",
        "text_col = 'text'\n",
        "data[text_col] = data[text_col].apply(remove_html).apply(remove_emoji).apply(remove_stopwords).apply(clean_str).apply(lemmatize_text)\n",
        "data = data.fillna('')\n",
        "\n",
        "project = 'pytorch'\n",
        "REPEAT = 25\n",
        "out_csv_name = f'/content/{project}_OptimizedNB_Count.csv'\n",
        "\n",
        "accuracies, precisions, recalls, f1_scores, auc_values = [], [], [], [], []\n",
        "\n",
        "for repeated_time in range(REPEAT):\n",
        "    print(f\"Running experiment {repeated_time + 1}/{REPEAT}...\")\n",
        "    indices = np.arange(data.shape[0])\n",
        "    train_index, test_index = train_test_split(indices, test_size=0.2, random_state=repeated_time)\n",
        "\n",
        "    train_texts = data[text_col].iloc[train_index]\n",
        "    test_texts = data[text_col].iloc[test_index]\n",
        "    y_train = data['sentiment'].iloc[train_index]\n",
        "    y_test = data['sentiment'].iloc[test_index]\n",
        "\n",
        "    vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=10000, min_df=3, max_df=0.85)\n",
        "    X_train = vectorizer.fit_transform(train_texts)\n",
        "    X_test = vectorizer.transform(test_texts)\n",
        "\n",
        "    smote = SMOTE(random_state=repeated_time)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    clf = MultinomialNB()  # Changed from ComplementNB() to MultinomialNB()\n",
        "    param_grid = {'alpha': np.logspace(-3, 1, 20)}\n",
        "    grid = GridSearchCV(clf, param_grid, cv=5, scoring='f1_macro')\n",
        "    grid.fit(X_train_balanced, y_train_balanced)  # Removed .toarray()\n",
        "    best_clf = grid.best_estimator_\n",
        "\n",
        "    best_clf.fit(X_train_balanced, y_train_balanced)  # Removed .toarray()\n",
        "    y_pred = best_clf.predict(X_test)  # Removed .toarray()\n",
        "    y_pred_proba = best_clf.predict_proba(X_test)  # Removed .toarray()\n",
        "\n",
        "    accuracies.append(accuracy_score(y_test, y_pred))\n",
        "    precisions.append(precision_score(y_test, y_pred, average='macro'))\n",
        "    recalls.append(recall_score(y_test, y_pred, average='macro'))\n",
        "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1], pos_label=1)\n",
        "    auc_values.append(auc(fpr, tpr))\n",
        "\n",
        "final_accuracy = np.mean(accuracies)\n",
        "final_precision = np.mean(precisions)\n",
        "final_recall = np.mean(recalls)\n",
        "final_f1 = np.mean(f1_scores)\n",
        "final_auc = np.mean(auc_values)\n",
        "\n",
        "print(\"=== Multinomial Naive Bayes + CountVectorizer Results ===\")\n",
        "print(f\"Average Accuracy:      {final_accuracy:.4f}\")\n",
        "print(f\"Average Precision:     {final_precision:.4f}\")\n",
        "print(f\"Average Recall:        {final_recall:.4f}\")\n",
        "print(f\"Average F1 score:      {final_f1:.4f}\")\n",
        "print(f\"Average AUC:           {final_auc:.4f}\")\n",
        "\n",
        "\n",
        "df_log = pd.DataFrame({\n",
        "    'repeated_times': [REPEAT], 'Accuracy': [final_accuracy],\n",
        "    'Precision': [final_precision], 'Recall': [final_recall], 'F1': [final_f1], 'AUC': [final_auc],\n",
        "    'CV_list(AUC)': [str(auc_values)]\n",
        "})\n",
        "df_log.to_csv(out_csv_name, mode='a', header=not pd.io.common.file_exists(out_csv_name), index=False)\n",
        "print(f\"Results saved to: {out_csv_name}\")"
      ]
    }
  ]
}